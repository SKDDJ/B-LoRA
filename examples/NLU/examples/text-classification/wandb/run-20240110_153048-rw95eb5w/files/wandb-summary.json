{"Training Loss": 0.331423282623291, "_timestamp": 1704872150.912806, "Learning rate": 0.0003898062877103843, "epoch": 6, "_runtime": 302.7293131351471, "_step": 1800, "gradients/classifier.out_proj.bias": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-498.5, -482.91796875, -467.3359375, -451.75390625, -436.171875, -420.58984375, -405.0078125, -389.42578125, -373.84375, -358.26171875, -342.6796875, -327.09765625, -311.515625, -295.93359375, -280.3515625, -264.76953125, -249.1875, -233.60546875, -218.0234375, -202.44140625, -186.859375, -171.27734375, -155.6953125, -140.11328125, -124.53125, -108.94921875, -93.3671875, -77.78515625, -62.203125, -46.62109375, -31.0390625, -15.45703125, 0.125, 15.70703125, 31.2890625, 46.87109375, 62.453125, 78.03515625, 93.6171875, 109.19921875, 124.78125, 140.36328125, 155.9453125, 171.52734375, 187.109375, 202.69140625, 218.2734375, 233.85546875, 249.4375, 265.01953125, 280.6015625, 296.18359375, 311.765625, 327.34765625, 342.9296875, 358.51171875, 374.09375, 389.67578125, 405.2578125, 420.83984375, 436.421875, 452.00390625, 467.5859375, 483.16796875, 498.75]}, "gradients/roberta.encoder.layer.11.attention.self.value.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 3.0, 7.0, 2.0, 8.0, 10.0, 5.0, 4.0, 18.0, 15.0, 33.0, 34.0, 42.0, 60.0, 77.0, 115.0, 146.0, 185.0, 213.0, 283.0, 356.0, 410.0, 417.0, 415.0, 486.0, 432.0, 431.0, 379.0, 343.0, 267.0, 210.0, 175.0, 138.0, 99.0, 81.0, 61.0, 52.0, 27.0, 26.0, 20.0, 16.0, 7.0, 6.0, 8.0, 6.0, 6.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0], "bins": [-163.75, -158.990234375, -154.23046875, -149.470703125, -144.7109375, -139.951171875, -135.19140625, -130.431640625, -125.671875, -120.912109375, -116.15234375, -111.392578125, -106.6328125, -101.873046875, -97.11328125, -92.353515625, -87.59375, -82.833984375, -78.07421875, -73.314453125, -68.5546875, -63.794921875, -59.03515625, -54.275390625, -49.515625, -44.755859375, -39.99609375, -35.236328125, -30.4765625, -25.716796875, -20.95703125, -16.197265625, -11.4375, -6.677734375, -1.91796875, 2.841796875, 7.6015625, 12.361328125, 17.12109375, 21.880859375, 26.640625, 31.400390625, 36.16015625, 40.919921875, 45.6796875, 50.439453125, 55.19921875, 59.958984375, 64.71875, 69.478515625, 74.23828125, 78.998046875, 83.7578125, 88.517578125, 93.27734375, 98.037109375, 102.796875, 107.556640625, 112.31640625, 117.076171875, 121.8359375, 126.595703125, 131.35546875, 136.115234375, 140.875]}, "gradients/roberta.encoder.layer.11.attention.self.value.lora_A": {"_type": "histogram", "values": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 4.0, 14.0, 29.0, 903.0, 4549.0, 590.0, 24.0, 8.0, 4.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-2422.0, -2349.5625, -2277.125, -2204.6875, -2132.25, -2059.8125, -1987.375, -1914.9375, -1842.5, -1770.0625, -1697.625, -1625.1875, -1552.75, -1480.3125, -1407.875, -1335.4375, -1263.0, -1190.5625, -1118.125, -1045.6875, -973.25, -900.8125, -828.375, -755.9375, -683.5, -611.0625, -538.625, -466.1875, -393.75, -321.3125, -248.875, -176.4375, -104.0, -31.5625, 40.875, 113.3125, 185.75, 258.1875, 330.625, 403.0625, 475.5, 547.9375, 620.375, 692.8125, 765.25, 837.6875, 910.125, 982.5625, 1055.0, 1127.4375, 1199.875, 1272.3125, 1344.75, 1417.1875, 1489.625, 1562.0625, 1634.5, 1706.9375, 1779.375, 1851.8125, 1924.25, 1996.6875, 2069.125, 2141.5625, 2214.0]}, "gradients/roberta.encoder.layer.11.attention.self.query.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 3.0, 5.0, 8.0, 7.0, 8.0, 16.0, 15.0, 9.0, 9.0, 33.0, 24.0, 48.0, 73.0, 130.0, 174.0, 232.0, 338.0, 448.0, 602.0, 679.0, 683.0, 692.0, 560.0, 409.0, 245.0, 205.0, 135.0, 123.0, 62.0, 39.0, 33.0, 14.0, 6.0, 13.0, 19.0, 11.0, 10.0, 5.0, 0.0, 2.0, 3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 2.0], "bins": [-179.75, -174.005859375, -168.26171875, -162.517578125, -156.7734375, -151.029296875, -145.28515625, -139.541015625, -133.796875, -128.052734375, -122.30859375, -116.564453125, -110.8203125, -105.076171875, -99.33203125, -93.587890625, -87.84375, -82.099609375, -76.35546875, -70.611328125, -64.8671875, -59.123046875, -53.37890625, -47.634765625, -41.890625, -36.146484375, -30.40234375, -24.658203125, -18.9140625, -13.169921875, -7.42578125, -1.681640625, 4.0625, 9.806640625, 15.55078125, 21.294921875, 27.0390625, 32.783203125, 38.52734375, 44.271484375, 50.015625, 55.759765625, 61.50390625, 67.248046875, 72.9921875, 78.736328125, 84.48046875, 90.224609375, 95.96875, 101.712890625, 107.45703125, 113.201171875, 118.9453125, 124.689453125, 130.43359375, 136.177734375, 141.921875, 147.666015625, 153.41015625, 159.154296875, 164.8984375, 170.642578125, 176.38671875, 182.130859375, 187.875]}, "gradients/roberta.encoder.layer.11.attention.self.query.lora_A": {"_type": "histogram", "values": [2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 6.0, 121.0, 5962.0, 26.0, 10.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 3.0], "bins": [-106.625, -103.3349609375, -100.044921875, -96.7548828125, -93.46484375, -90.1748046875, -86.884765625, -83.5947265625, -80.3046875, -77.0146484375, -73.724609375, -70.4345703125, -67.14453125, -63.8544921875, -60.564453125, -57.2744140625, -53.984375, -50.6943359375, -47.404296875, -44.1142578125, -40.82421875, -37.5341796875, -34.244140625, -30.9541015625, -27.6640625, -24.3740234375, -21.083984375, -17.7939453125, -14.50390625, -11.2138671875, -7.923828125, -4.6337890625, -1.34375, 1.9462890625, 5.236328125, 8.5263671875, 11.81640625, 15.1064453125, 18.396484375, 21.6865234375, 24.9765625, 28.2666015625, 31.556640625, 34.8466796875, 38.13671875, 41.4267578125, 44.716796875, 48.0068359375, 51.296875, 54.5869140625, 57.876953125, 61.1669921875, 64.45703125, 67.7470703125, 71.037109375, 74.3271484375, 77.6171875, 80.9072265625, 84.197265625, 87.4873046875, 90.77734375, 94.0673828125, 97.357421875, 100.6474609375, 103.9375]}, "gradients/roberta.encoder.layer.10.attention.self.value.lora_B": {"_type": "histogram", "values": [2.0, 2.0, 3.0, 3.0, 6.0, 7.0, 10.0, 6.0, 17.0, 25.0, 22.0, 32.0, 29.0, 53.0, 42.0, 49.0, 80.0, 89.0, 138.0, 141.0, 174.0, 232.0, 294.0, 341.0, 402.0, 393.0, 474.0, 447.0, 415.0, 369.0, 338.0, 269.0, 227.0, 185.0, 140.0, 142.0, 100.0, 102.0, 66.0, 47.0, 34.0, 39.0, 31.0, 15.0, 20.0, 22.0, 16.0, 9.0, 11.0, 5.0, 4.0, 5.0, 7.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 0.0, 1.0, 0.0, 1.0], "bins": [-98.3125, -94.6826171875, -91.052734375, -87.4228515625, -83.79296875, -80.1630859375, -76.533203125, -72.9033203125, -69.2734375, -65.6435546875, -62.013671875, -58.3837890625, -54.75390625, -51.1240234375, -47.494140625, -43.8642578125, -40.234375, -36.6044921875, -32.974609375, -29.3447265625, -25.71484375, -22.0849609375, -18.455078125, -14.8251953125, -11.1953125, -7.5654296875, -3.935546875, -0.3056640625, 3.32421875, 6.9541015625, 10.583984375, 14.2138671875, 17.84375, 21.4736328125, 25.103515625, 28.7333984375, 32.36328125, 35.9931640625, 39.623046875, 43.2529296875, 46.8828125, 50.5126953125, 54.142578125, 57.7724609375, 61.40234375, 65.0322265625, 68.662109375, 72.2919921875, 75.921875, 79.5517578125, 83.181640625, 86.8115234375, 90.44140625, 94.0712890625, 97.701171875, 101.3310546875, 104.9609375, 108.5908203125, 112.220703125, 115.8505859375, 119.48046875, 123.1103515625, 126.740234375, 130.3701171875, 134.0]}, "gradients/roberta.encoder.layer.10.attention.self.value.lora_A": {"_type": "histogram", "values": [3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 11.0, 16.0, 77.0, 1254.0, 4291.0, 433.0, 32.0, 6.0, 5.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], "bins": [-4148.0, -4030.5625, -3913.125, -3795.6875, -3678.25, -3560.8125, -3443.375, -3325.9375, -3208.5, -3091.0625, -2973.625, -2856.1875, -2738.75, -2621.3125, -2503.875, -2386.4375, -2269.0, -2151.5625, -2034.125, -1916.6875, -1799.25, -1681.8125, -1564.375, -1446.9375, -1329.5, -1212.0625, -1094.625, -977.1875, -859.75, -742.3125, -624.875, -507.4375, -390.0, -272.5625, -155.125, -37.6875, 79.75, 197.1875, 314.625, 432.0625, 549.5, 666.9375, 784.375, 901.8125, 1019.25, 1136.6875, 1254.125, 1371.5625, 1489.0, 1606.4375, 1723.875, 1841.3125, 1958.75, 2076.1875, 2193.625, 2311.0625, 2428.5, 2545.9375, 2663.375, 2780.8125, 2898.25, 3015.6875, 3133.125, 3250.5625, 3368.0]}, "gradients/roberta.encoder.layer.10.attention.self.query.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 2.0, 5.0, 1.0, 2.0, 2.0, 10.0, 3.0, 15.0, 12.0, 15.0, 14.0, 26.0, 35.0, 38.0, 56.0, 66.0, 95.0, 151.0, 196.0, 244.0, 356.0, 353.0, 449.0, 681.0, 816.0, 554.0, 407.0, 341.0, 301.0, 242.0, 164.0, 132.0, 83.0, 68.0, 45.0, 30.0, 31.0, 20.0, 16.0, 15.0, 7.0, 13.0, 5.0, 6.0, 3.0, 3.0, 3.0, 1.0, 1.0, 3.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0], "bins": [-229.375, -222.26953125, -215.1640625, -208.05859375, -200.953125, -193.84765625, -186.7421875, -179.63671875, -172.53125, -165.42578125, -158.3203125, -151.21484375, -144.109375, -137.00390625, -129.8984375, -122.79296875, -115.6875, -108.58203125, -101.4765625, -94.37109375, -87.265625, -80.16015625, -73.0546875, -65.94921875, -58.84375, -51.73828125, -44.6328125, -37.52734375, -30.421875, -23.31640625, -16.2109375, -9.10546875, -2.0, 5.10546875, 12.2109375, 19.31640625, 26.421875, 33.52734375, 40.6328125, 47.73828125, 54.84375, 61.94921875, 69.0546875, 76.16015625, 83.265625, 90.37109375, 97.4765625, 104.58203125, 111.6875, 118.79296875, 125.8984375, 133.00390625, 140.109375, 147.21484375, 154.3203125, 161.42578125, 168.53125, 175.63671875, 182.7421875, 189.84765625, 196.953125, 204.05859375, 211.1640625, 218.26953125, 225.375]}, "gradients/roberta.encoder.layer.10.attention.self.query.lora_A": {"_type": "histogram", "values": [3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 23.0, 169.0, 3278.0, 2528.0, 97.0, 18.0, 7.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0], "bins": [-325.5, -315.30078125, -305.1015625, -294.90234375, -284.703125, -274.50390625, -264.3046875, -254.10546875, -243.90625, -233.70703125, -223.5078125, -213.30859375, -203.109375, -192.91015625, -182.7109375, -172.51171875, -162.3125, -152.11328125, -141.9140625, -131.71484375, -121.515625, -111.31640625, -101.1171875, -90.91796875, -80.71875, -70.51953125, -60.3203125, -50.12109375, -39.921875, -29.72265625, -19.5234375, -9.32421875, 0.875, 11.07421875, 21.2734375, 31.47265625, 41.671875, 51.87109375, 62.0703125, 72.26953125, 82.46875, 92.66796875, 102.8671875, 113.06640625, 123.265625, 133.46484375, 143.6640625, 153.86328125, 164.0625, 174.26171875, 184.4609375, 194.66015625, 204.859375, 215.05859375, 225.2578125, 235.45703125, 245.65625, 255.85546875, 266.0546875, 276.25390625, 286.453125, 296.65234375, 306.8515625, 317.05078125, 327.25]}, "gradients/roberta.encoder.layer.9.attention.self.value.lora_B": {"_type": "histogram", "values": [2.0, 0.0, 0.0, 3.0, 1.0, 0.0, 2.0, 2.0, 2.0, 10.0, 8.0, 7.0, 10.0, 18.0, 18.0, 28.0, 45.0, 50.0, 77.0, 101.0, 131.0, 245.0, 386.0, 618.0, 785.0, 988.0, 877.0, 532.0, 417.0, 278.0, 167.0, 91.0, 79.0, 39.0, 28.0, 29.0, 20.0, 14.0, 9.0, 7.0, 8.0, 4.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0], "bins": [-273.0, -262.30859375, -251.6171875, -240.92578125, -230.234375, -219.54296875, -208.8515625, -198.16015625, -187.46875, -176.77734375, -166.0859375, -155.39453125, -144.703125, -134.01171875, -123.3203125, -112.62890625, -101.9375, -91.24609375, -80.5546875, -69.86328125, -59.171875, -48.48046875, -37.7890625, -27.09765625, -16.40625, -5.71484375, 4.9765625, 15.66796875, 26.359375, 37.05078125, 47.7421875, 58.43359375, 69.125, 79.81640625, 90.5078125, 101.19921875, 111.890625, 122.58203125, 133.2734375, 143.96484375, 154.65625, 165.34765625, 176.0390625, 186.73046875, 197.421875, 208.11328125, 218.8046875, 229.49609375, 240.1875, 250.87890625, 261.5703125, 272.26171875, 282.953125, 293.64453125, 304.3359375, 315.02734375, 325.71875, 336.41015625, 347.1015625, 357.79296875, 368.484375, 379.17578125, 389.8671875, 400.55859375, 411.25]}, "gradients/roberta.encoder.layer.9.attention.self.value.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 3.0, 2.0, 4.0, 13.0, 325.0, 4768.0, 965.0, 40.0, 4.0, 4.0, 4.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0], "bins": [-1537.0, -1495.125, -1453.25, -1411.375, -1369.5, -1327.625, -1285.75, -1243.875, -1202.0, -1160.125, -1118.25, -1076.375, -1034.5, -992.625, -950.75, -908.875, -867.0, -825.125, -783.25, -741.375, -699.5, -657.625, -615.75, -573.875, -532.0, -490.125, -448.25, -406.375, -364.5, -322.625, -280.75, -238.875, -197.0, -155.125, -113.25, -71.375, -29.5, 12.375, 54.25, 96.125, 138.0, 179.875, 221.75, 263.625, 305.5, 347.375, 389.25, 431.125, 473.0, 514.875, 556.75, 598.625, 640.5, 682.375, 724.25, 766.125, 808.0, 849.875, 891.75, 933.625, 975.5, 1017.375, 1059.25, 1101.125, 1143.0]}, "gradients/roberta.encoder.layer.9.attention.self.query.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 7.0, 6.0, 4.0, 7.0, 6.0, 17.0, 14.0, 33.0, 43.0, 77.0, 113.0, 197.0, 381.0, 666.0, 1780.0, 1394.0, 594.0, 329.0, 180.0, 97.0, 50.0, 45.0, 29.0, 18.0, 12.0, 11.0, 4.0, 3.0, 4.0, 1.0, 3.0, 0.0, 1.0, 3.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0], "bins": [-395.5, -383.46875, -371.4375, -359.40625, -347.375, -335.34375, -323.3125, -311.28125, -299.25, -287.21875, -275.1875, -263.15625, -251.125, -239.09375, -227.0625, -215.03125, -203.0, -190.96875, -178.9375, -166.90625, -154.875, -142.84375, -130.8125, -118.78125, -106.75, -94.71875, -82.6875, -70.65625, -58.625, -46.59375, -34.5625, -22.53125, -10.5, 1.53125, 13.5625, 25.59375, 37.625, 49.65625, 61.6875, 73.71875, 85.75, 97.78125, 109.8125, 121.84375, 133.875, 145.90625, 157.9375, 169.96875, 182.0, 194.03125, 206.0625, 218.09375, 230.125, 242.15625, 254.1875, 266.21875, 278.25, 290.28125, 302.3125, 314.34375, 326.375, 338.40625, 350.4375, 362.46875, 374.5]}, "gradients/roberta.encoder.layer.9.attention.self.query.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 3.0, 11.0, 72.0, 1788.0, 4022.0, 211.0, 12.0, 6.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0], "bins": [-1056.0, -1029.71875, -1003.4375, -977.15625, -950.875, -924.59375, -898.3125, -872.03125, -845.75, -819.46875, -793.1875, -766.90625, -740.625, -714.34375, -688.0625, -661.78125, -635.5, -609.21875, -582.9375, -556.65625, -530.375, -504.09375, -477.8125, -451.53125, -425.25, -398.96875, -372.6875, -346.40625, -320.125, -293.84375, -267.5625, -241.28125, -215.0, -188.71875, -162.4375, -136.15625, -109.875, -83.59375, -57.3125, -31.03125, -4.75, 21.53125, 47.8125, 74.09375, 100.375, 126.65625, 152.9375, 179.21875, 205.5, 231.78125, 258.0625, 284.34375, 310.625, 336.90625, 363.1875, 389.46875, 415.75, 442.03125, 468.3125, 494.59375, 520.875, 547.15625, 573.4375, 599.71875, 626.0]}, "gradients/roberta.encoder.layer.8.attention.self.value.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 1.0, 1.0, 4.0, 3.0, 5.0, 9.0, 13.0, 12.0, 16.0, 26.0, 47.0, 60.0, 94.0, 104.0, 181.0, 248.0, 375.0, 447.0, 604.0, 674.0, 722.0, 709.0, 528.0, 354.0, 284.0, 180.0, 131.0, 101.0, 68.0, 35.0, 30.0, 24.0, 11.0, 13.0, 4.0, 5.0, 6.0, 1.0, 5.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-277.75, -265.22265625, -252.6953125, -240.16796875, -227.640625, -215.11328125, -202.5859375, -190.05859375, -177.53125, -165.00390625, -152.4765625, -139.94921875, -127.421875, -114.89453125, -102.3671875, -89.83984375, -77.3125, -64.78515625, -52.2578125, -39.73046875, -27.203125, -14.67578125, -2.1484375, 10.37890625, 22.90625, 35.43359375, 47.9609375, 60.48828125, 73.015625, 85.54296875, 98.0703125, 110.59765625, 123.125, 135.65234375, 148.1796875, 160.70703125, 173.234375, 185.76171875, 198.2890625, 210.81640625, 223.34375, 235.87109375, 248.3984375, 260.92578125, 273.453125, 285.98046875, 298.5078125, 311.03515625, 323.5625, 336.08984375, 348.6171875, 361.14453125, 373.671875, 386.19921875, 398.7265625, 411.25390625, 423.78125, 436.30859375, 448.8359375, 461.36328125, 473.890625, 486.41796875, 498.9453125, 511.47265625, 524.0]}, "gradients/roberta.encoder.layer.8.attention.self.value.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 5.0, 41.0, 6027.0, 55.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0], "bins": [-2112.0, -2018.28125, -1924.5625, -1830.84375, -1737.125, -1643.40625, -1549.6875, -1455.96875, -1362.25, -1268.53125, -1174.8125, -1081.09375, -987.375, -893.65625, -799.9375, -706.21875, -612.5, -518.78125, -425.0625, -331.34375, -237.625, -143.90625, -50.1875, 43.53125, 137.25, 230.96875, 324.6875, 418.40625, 512.125, 605.84375, 699.5625, 793.28125, 887.0, 980.71875, 1074.4375, 1168.15625, 1261.875, 1355.59375, 1449.3125, 1543.03125, 1636.75, 1730.46875, 1824.1875, 1917.90625, 2011.625, 2105.34375, 2199.0625, 2292.78125, 2386.5, 2480.21875, 2573.9375, 2667.65625, 2761.375, 2855.09375, 2948.8125, 3042.53125, 3136.25, 3229.96875, 3323.6875, 3417.40625, 3511.125, 3604.84375, 3698.5625, 3792.28125, 3886.0]}, "gradients/roberta.encoder.layer.8.attention.self.query.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 3.0, 8.0, 6.0, 7.0, 8.0, 24.0, 28.0, 30.0, 59.0, 167.0, 770.0, 3308.0, 1267.0, 227.0, 78.0, 46.0, 28.0, 28.0, 15.0, 7.0, 7.0, 7.0, 4.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0], "bins": [-906.0, -876.4375, -846.875, -817.3125, -787.75, -758.1875, -728.625, -699.0625, -669.5, -639.9375, -610.375, -580.8125, -551.25, -521.6875, -492.125, -462.5625, -433.0, -403.4375, -373.875, -344.3125, -314.75, -285.1875, -255.625, -226.0625, -196.5, -166.9375, -137.375, -107.8125, -78.25, -48.6875, -19.125, 10.4375, 40.0, 69.5625, 99.125, 128.6875, 158.25, 187.8125, 217.375, 246.9375, 276.5, 306.0625, 335.625, 365.1875, 394.75, 424.3125, 453.875, 483.4375, 513.0, 542.5625, 572.125, 601.6875, 631.25, 660.8125, 690.375, 719.9375, 749.5, 779.0625, 808.625, 838.1875, 867.75, 897.3125, 926.875, 956.4375, 986.0]}, "gradients/roberta.encoder.layer.8.attention.self.query.lora_A": {"_type": "histogram", "values": [3.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 2.0, 4.0, 11.0, 134.0, 2608.0, 3182.0, 153.0, 19.0, 8.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-651.5, -627.4140625, -603.328125, -579.2421875, -555.15625, -531.0703125, -506.984375, -482.8984375, -458.8125, -434.7265625, -410.640625, -386.5546875, -362.46875, -338.3828125, -314.296875, -290.2109375, -266.125, -242.0390625, -217.953125, -193.8671875, -169.78125, -145.6953125, -121.609375, -97.5234375, -73.4375, -49.3515625, -25.265625, -1.1796875, 22.90625, 46.9921875, 71.078125, 95.1640625, 119.25, 143.3359375, 167.421875, 191.5078125, 215.59375, 239.6796875, 263.765625, 287.8515625, 311.9375, 336.0234375, 360.109375, 384.1953125, 408.28125, 432.3671875, 456.453125, 480.5390625, 504.625, 528.7109375, 552.796875, 576.8828125, 600.96875, 625.0546875, 649.140625, 673.2265625, 697.3125, 721.3984375, 745.484375, 769.5703125, 793.65625, 817.7421875, 841.828125, 865.9140625, 890.0]}, "gradients/roberta.encoder.layer.7.attention.self.value.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 5.0, 3.0, 7.0, 17.0, 36.0, 47.0, 88.0, 147.0, 277.0, 568.0, 996.0, 1337.0, 1132.0, 692.0, 372.0, 182.0, 91.0, 54.0, 27.0, 20.0, 10.0, 6.0, 7.0, 2.0, 6.0, 1.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0], "bins": [-970.0, -945.5390625, -921.078125, -896.6171875, -872.15625, -847.6953125, -823.234375, -798.7734375, -774.3125, -749.8515625, -725.390625, -700.9296875, -676.46875, -652.0078125, -627.546875, -603.0859375, -578.625, -554.1640625, -529.703125, -505.2421875, -480.78125, -456.3203125, -431.859375, -407.3984375, -382.9375, -358.4765625, -334.015625, -309.5546875, -285.09375, -260.6328125, -236.171875, -211.7109375, -187.25, -162.7890625, -138.328125, -113.8671875, -89.40625, -64.9453125, -40.484375, -16.0234375, 8.4375, 32.8984375, 57.359375, 81.8203125, 106.28125, 130.7421875, 155.203125, 179.6640625, 204.125, 228.5859375, 253.046875, 277.5078125, 301.96875, 326.4296875, 350.890625, 375.3515625, 399.8125, 424.2734375, 448.734375, 473.1953125, 497.65625, 522.1171875, 546.578125, 571.0390625, 595.5]}, "gradients/roberta.encoder.layer.7.attention.self.value.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 1.0, 9.0, 1518.0, 4578.0, 19.0, 2.0, 2.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-3744.0, -3637.3125, -3530.625, -3423.9375, -3317.25, -3210.5625, -3103.875, -2997.1875, -2890.5, -2783.8125, -2677.125, -2570.4375, -2463.75, -2357.0625, -2250.375, -2143.6875, -2037.0, -1930.3125, -1823.625, -1716.9375, -1610.25, -1503.5625, -1396.875, -1290.1875, -1183.5, -1076.8125, -970.125, -863.4375, -756.75, -650.0625, -543.375, -436.6875, -330.0, -223.3125, -116.625, -9.9375, 96.75, 203.4375, 310.125, 416.8125, 523.5, 630.1875, 736.875, 843.5625, 950.25, 1056.9375, 1163.625, 1270.3125, 1377.0, 1483.6875, 1590.375, 1697.0625, 1803.75, 1910.4375, 2017.125, 2123.8125, 2230.5, 2337.1875, 2443.875, 2550.5625, 2657.25, 2763.9375, 2870.625, 2977.3125, 3084.0]}, "gradients/roberta.encoder.layer.7.attention.self.query.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 6.0, 5.0, 1.0, 4.0, 3.0, 7.0, 7.0, 15.0, 20.0, 23.0, 54.0, 88.0, 138.0, 314.0, 778.0, 2261.0, 1373.0, 522.0, 211.0, 139.0, 60.0, 31.0, 17.0, 22.0, 7.0, 8.0, 3.0, 2.0, 1.0, 3.0, 0.0, 3.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-351.0, -338.3515625, -325.703125, -313.0546875, -300.40625, -287.7578125, -275.109375, -262.4609375, -249.8125, -237.1640625, -224.515625, -211.8671875, -199.21875, -186.5703125, -173.921875, -161.2734375, -148.625, -135.9765625, -123.328125, -110.6796875, -98.03125, -85.3828125, -72.734375, -60.0859375, -47.4375, -34.7890625, -22.140625, -9.4921875, 3.15625, 15.8046875, 28.453125, 41.1015625, 53.75, 66.3984375, 79.046875, 91.6953125, 104.34375, 116.9921875, 129.640625, 142.2890625, 154.9375, 167.5859375, 180.234375, 192.8828125, 205.53125, 218.1796875, 230.828125, 243.4765625, 256.125, 268.7734375, 281.421875, 294.0703125, 306.71875, 319.3671875, 332.015625, 344.6640625, 357.3125, 369.9609375, 382.609375, 395.2578125, 407.90625, 420.5546875, 433.203125, 445.8515625, 458.5]}, "gradients/roberta.encoder.layer.7.attention.self.query.lora_A": {"_type": "histogram", "values": [2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 14.0, 52.0, 280.0, 3669.0, 1891.0, 180.0, 34.0, 10.0, 3.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-2084.0, -2024.28125, -1964.5625, -1904.84375, -1845.125, -1785.40625, -1725.6875, -1665.96875, -1606.25, -1546.53125, -1486.8125, -1427.09375, -1367.375, -1307.65625, -1247.9375, -1188.21875, -1128.5, -1068.78125, -1009.0625, -949.34375, -889.625, -829.90625, -770.1875, -710.46875, -650.75, -591.03125, -531.3125, -471.59375, -411.875, -352.15625, -292.4375, -232.71875, -173.0, -113.28125, -53.5625, 6.15625, 65.875, 125.59375, 185.3125, 245.03125, 304.75, 364.46875, 424.1875, 483.90625, 543.625, 603.34375, 663.0625, 722.78125, 782.5, 842.21875, 901.9375, 961.65625, 1021.375, 1081.09375, 1140.8125, 1200.53125, 1260.25, 1319.96875, 1379.6875, 1439.40625, 1499.125, 1558.84375, 1618.5625, 1678.28125, 1738.0]}, "gradients/roberta.encoder.layer.6.attention.self.value.lora_B": {"_type": "histogram", "values": [2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 0.0, 1.0, 0.0, 7.0, 6.0, 3.0, 4.0, 7.0, 6.0, 8.0, 9.0, 16.0, 19.0, 28.0, 45.0, 46.0, 73.0, 99.0, 115.0, 175.0, 234.0, 322.0, 428.0, 510.0, 626.0, 639.0, 652.0, 544.0, 397.0, 282.0, 237.0, 163.0, 117.0, 88.0, 53.0, 41.0, 32.0, 21.0, 17.0, 12.0, 8.0, 8.0, 9.0, 9.0, 3.0, 3.0, 1.0, 2.0, 1.0, 4.0, 2.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0], "bins": [-313.5, -303.54296875, -293.5859375, -283.62890625, -273.671875, -263.71484375, -253.7578125, -243.80078125, -233.84375, -223.88671875, -213.9296875, -203.97265625, -194.015625, -184.05859375, -174.1015625, -164.14453125, -154.1875, -144.23046875, -134.2734375, -124.31640625, -114.359375, -104.40234375, -94.4453125, -84.48828125, -74.53125, -64.57421875, -54.6171875, -44.66015625, -34.703125, -24.74609375, -14.7890625, -4.83203125, 5.125, 15.08203125, 25.0390625, 34.99609375, 44.953125, 54.91015625, 64.8671875, 74.82421875, 84.78125, 94.73828125, 104.6953125, 114.65234375, 124.609375, 134.56640625, 144.5234375, 154.48046875, 164.4375, 174.39453125, 184.3515625, 194.30859375, 204.265625, 214.22265625, 224.1796875, 234.13671875, 244.09375, 254.05078125, 264.0078125, 273.96484375, 283.921875, 293.87890625, 303.8359375, 313.79296875, 323.75]}, "gradients/roberta.encoder.layer.6.attention.self.value.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 8.0, 8.0, 5779.0, 328.0, 4.0, 4.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0], "bins": [-2172.0, -2084.34375, -1996.6875, -1909.03125, -1821.375, -1733.71875, -1646.0625, -1558.40625, -1470.75, -1383.09375, -1295.4375, -1207.78125, -1120.125, -1032.46875, -944.8125, -857.15625, -769.5, -681.84375, -594.1875, -506.53125, -418.875, -331.21875, -243.5625, -155.90625, -68.25, 19.40625, 107.0625, 194.71875, 282.375, 370.03125, 457.6875, 545.34375, 633.0, 720.65625, 808.3125, 895.96875, 983.625, 1071.28125, 1158.9375, 1246.59375, 1334.25, 1421.90625, 1509.5625, 1597.21875, 1684.875, 1772.53125, 1860.1875, 1947.84375, 2035.5, 2123.15625, 2210.8125, 2298.46875, 2386.125, 2473.78125, 2561.4375, 2649.09375, 2736.75, 2824.40625, 2912.0625, 2999.71875, 3087.375, 3175.03125, 3262.6875, 3350.34375, 3438.0]}, "gradients/roberta.encoder.layer.6.attention.self.query.lora_B": {"_type": "histogram", "values": [2.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 5.0, 1.0, 5.0, 10.0, 9.0, 20.0, 23.0, 30.0, 46.0, 79.0, 163.0, 276.0, 587.0, 1217.0, 1634.0, 984.0, 431.0, 273.0, 106.0, 56.0, 46.0, 24.0, 18.0, 15.0, 11.0, 13.0, 7.0, 9.0, 4.0, 3.0, 1.0, 5.0, 5.0, 2.0, 0.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 0.0, 1.0], "bins": [-796.5, -772.59375, -748.6875, -724.78125, -700.875, -676.96875, -653.0625, -629.15625, -605.25, -581.34375, -557.4375, -533.53125, -509.625, -485.71875, -461.8125, -437.90625, -414.0, -390.09375, -366.1875, -342.28125, -318.375, -294.46875, -270.5625, -246.65625, -222.75, -198.84375, -174.9375, -151.03125, -127.125, -103.21875, -79.3125, -55.40625, -31.5, -7.59375, 16.3125, 40.21875, 64.125, 88.03125, 111.9375, 135.84375, 159.75, 183.65625, 207.5625, 231.46875, 255.375, 279.28125, 303.1875, 327.09375, 351.0, 374.90625, 398.8125, 422.71875, 446.625, 470.53125, 494.4375, 518.34375, 542.25, 566.15625, 590.0625, 613.96875, 637.875, 661.78125, 685.6875, 709.59375, 733.5]}, "gradients/roberta.encoder.layer.6.attention.self.query.lora_A": {"_type": "histogram", "values": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 3.0, 8.0, 18.0, 94.0, 729.0, 3774.0, 1303.0, 144.0, 36.0, 15.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0], "bins": [-516.0, -497.3203125, -478.640625, -459.9609375, -441.28125, -422.6015625, -403.921875, -385.2421875, -366.5625, -347.8828125, -329.203125, -310.5234375, -291.84375, -273.1640625, -254.484375, -235.8046875, -217.125, -198.4453125, -179.765625, -161.0859375, -142.40625, -123.7265625, -105.046875, -86.3671875, -67.6875, -49.0078125, -30.328125, -11.6484375, 7.03125, 25.7109375, 44.390625, 63.0703125, 81.75, 100.4296875, 119.109375, 137.7890625, 156.46875, 175.1484375, 193.828125, 212.5078125, 231.1875, 249.8671875, 268.546875, 287.2265625, 305.90625, 324.5859375, 343.265625, 361.9453125, 380.625, 399.3046875, 417.984375, 436.6640625, 455.34375, 474.0234375, 492.703125, 511.3828125, 530.0625, 548.7421875, 567.421875, 586.1015625, 604.78125, 623.4609375, 642.140625, 660.8203125, 679.5]}, "gradients/roberta.encoder.layer.5.attention.self.value.lora_B": {"_type": "histogram", "values": [3.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 1.0, 4.0, 3.0, 1.0, 2.0, 5.0, 7.0, 3.0, 14.0, 11.0, 9.0, 33.0, 28.0, 38.0, 36.0, 62.0, 79.0, 91.0, 135.0, 175.0, 220.0, 305.0, 416.0, 556.0, 697.0, 721.0, 602.0, 471.0, 362.0, 275.0, 179.0, 142.0, 113.0, 73.0, 62.0, 46.0, 38.0, 33.0, 16.0, 18.0, 15.0, 10.0, 8.0, 0.0, 7.0, 5.0, 2.0, 2.0, 1.0, 2.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 2.0], "bins": [-381.75, -369.8984375, -358.046875, -346.1953125, -334.34375, -322.4921875, -310.640625, -298.7890625, -286.9375, -275.0859375, -263.234375, -251.3828125, -239.53125, -227.6796875, -215.828125, -203.9765625, -192.125, -180.2734375, -168.421875, -156.5703125, -144.71875, -132.8671875, -121.015625, -109.1640625, -97.3125, -85.4609375, -73.609375, -61.7578125, -49.90625, -38.0546875, -26.203125, -14.3515625, -2.5, 9.3515625, 21.203125, 33.0546875, 44.90625, 56.7578125, 68.609375, 80.4609375, 92.3125, 104.1640625, 116.015625, 127.8671875, 139.71875, 151.5703125, 163.421875, 175.2734375, 187.125, 198.9765625, 210.828125, 222.6796875, 234.53125, 246.3828125, 258.234375, 270.0859375, 281.9375, 293.7890625, 305.640625, 317.4921875, 329.34375, 341.1953125, 353.046875, 364.8984375, 376.75]}, "gradients/roberta.encoder.layer.5.attention.self.value.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 4.0, 3.0, 4.0, 3.0, 28.0, 4700.0, 1371.0, 13.0, 3.0, 3.0, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-1464.0, -1418.03125, -1372.0625, -1326.09375, -1280.125, -1234.15625, -1188.1875, -1142.21875, -1096.25, -1050.28125, -1004.3125, -958.34375, -912.375, -866.40625, -820.4375, -774.46875, -728.5, -682.53125, -636.5625, -590.59375, -544.625, -498.65625, -452.6875, -406.71875, -360.75, -314.78125, -268.8125, -222.84375, -176.875, -130.90625, -84.9375, -38.96875, 7.0, 52.96875, 98.9375, 144.90625, 190.875, 236.84375, 282.8125, 328.78125, 374.75, 420.71875, 466.6875, 512.65625, 558.625, 604.59375, 650.5625, 696.53125, 742.5, 788.46875, 834.4375, 880.40625, 926.375, 972.34375, 1018.3125, 1064.28125, 1110.25, 1156.21875, 1202.1875, 1248.15625, 1294.125, 1340.09375, 1386.0625, 1432.03125, 1478.0]}, "gradients/roberta.encoder.layer.5.attention.self.query.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 4.0, 1.0, 7.0, 11.0, 16.0, 33.0, 79.0, 267.0, 1531.0, 3461.0, 485.0, 133.0, 57.0, 27.0, 9.0, 4.0, 6.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-953.5, -919.7421875, -885.984375, -852.2265625, -818.46875, -784.7109375, -750.953125, -717.1953125, -683.4375, -649.6796875, -615.921875, -582.1640625, -548.40625, -514.6484375, -480.890625, -447.1328125, -413.375, -379.6171875, -345.859375, -312.1015625, -278.34375, -244.5859375, -210.828125, -177.0703125, -143.3125, -109.5546875, -75.796875, -42.0390625, -8.28125, 25.4765625, 59.234375, 92.9921875, 126.75, 160.5078125, 194.265625, 228.0234375, 261.78125, 295.5390625, 329.296875, 363.0546875, 396.8125, 430.5703125, 464.328125, 498.0859375, 531.84375, 565.6015625, 599.359375, 633.1171875, 666.875, 700.6328125, 734.390625, 768.1484375, 801.90625, 835.6640625, 869.421875, 903.1796875, 936.9375, 970.6953125, 1004.453125, 1038.2109375, 1071.96875, 1105.7265625, 1139.484375, 1173.2421875, 1207.0]}, "gradients/roberta.encoder.layer.5.attention.self.query.lora_A": {"_type": "histogram", "values": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 2.0, 20.0, 141.0, 2435.0, 3267.0, 238.0, 19.0, 4.0, 3.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-1941.0, -1878.546875, -1816.09375, -1753.640625, -1691.1875, -1628.734375, -1566.28125, -1503.828125, -1441.375, -1378.921875, -1316.46875, -1254.015625, -1191.5625, -1129.109375, -1066.65625, -1004.203125, -941.75, -879.296875, -816.84375, -754.390625, -691.9375, -629.484375, -567.03125, -504.578125, -442.125, -379.671875, -317.21875, -254.765625, -192.3125, -129.859375, -67.40625, -4.953125, 57.5, 119.953125, 182.40625, 244.859375, 307.3125, 369.765625, 432.21875, 494.671875, 557.125, 619.578125, 682.03125, 744.484375, 806.9375, 869.390625, 931.84375, 994.296875, 1056.75, 1119.203125, 1181.65625, 1244.109375, 1306.5625, 1369.015625, 1431.46875, 1493.921875, 1556.375, 1618.828125, 1681.28125, 1743.734375, 1806.1875, 1868.640625, 1931.09375, 1993.546875, 2056.0]}, "gradients/roberta.encoder.layer.4.attention.self.value.lora_B": {"_type": "histogram", "values": [2.0, 0.0, 0.0, 1.0, 1.0, 0.0, 3.0, 4.0, 0.0, 5.0, 1.0, 3.0, 3.0, 3.0, 7.0, 5.0, 7.0, 8.0, 18.0, 21.0, 34.0, 34.0, 55.0, 78.0, 100.0, 137.0, 186.0, 236.0, 356.0, 382.0, 581.0, 654.0, 726.0, 582.0, 477.0, 336.0, 280.0, 206.0, 154.0, 109.0, 82.0, 60.0, 47.0, 30.0, 29.0, 24.0, 19.0, 19.0, 11.0, 9.0, 4.0, 3.0, 1.0, 3.0, 4.0, 2.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-424.75, -411.56640625, -398.3828125, -385.19921875, -372.015625, -358.83203125, -345.6484375, -332.46484375, -319.28125, -306.09765625, -292.9140625, -279.73046875, -266.546875, -253.36328125, -240.1796875, -226.99609375, -213.8125, -200.62890625, -187.4453125, -174.26171875, -161.078125, -147.89453125, -134.7109375, -121.52734375, -108.34375, -95.16015625, -81.9765625, -68.79296875, -55.609375, -42.42578125, -29.2421875, -16.05859375, -2.875, 10.30859375, 23.4921875, 36.67578125, 49.859375, 63.04296875, 76.2265625, 89.41015625, 102.59375, 115.77734375, 128.9609375, 142.14453125, 155.328125, 168.51171875, 181.6953125, 194.87890625, 208.0625, 221.24609375, 234.4296875, 247.61328125, 260.796875, 273.98046875, 287.1640625, 300.34765625, 313.53125, 326.71484375, 339.8984375, 353.08203125, 366.265625, 379.44921875, 392.6328125, 405.81640625, 419.0]}, "gradients/roberta.encoder.layer.4.attention.self.value.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 11.0, 4810.0, 1297.0, 4.0, 6.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0], "bins": [-5000.0, -4843.1875, -4686.375, -4529.5625, -4372.75, -4215.9375, -4059.125, -3902.3125, -3745.5, -3588.6875, -3431.875, -3275.0625, -3118.25, -2961.4375, -2804.625, -2647.8125, -2491.0, -2334.1875, -2177.375, -2020.5625, -1863.75, -1706.9375, -1550.125, -1393.3125, -1236.5, -1079.6875, -922.875, -766.0625, -609.25, -452.4375, -295.625, -138.8125, 18.0, 174.8125, 331.625, 488.4375, 645.25, 802.0625, 958.875, 1115.6875, 1272.5, 1429.3125, 1586.125, 1742.9375, 1899.75, 2056.5625, 2213.375, 2370.1875, 2527.0, 2683.8125, 2840.625, 2997.4375, 3154.25, 3311.0625, 3467.875, 3624.6875, 3781.5, 3938.3125, 4095.125, 4251.9375, 4408.75, 4565.5625, 4722.375, 4879.1875, 5036.0]}, "gradients/roberta.encoder.layer.4.attention.self.query.lora_B": {"_type": "histogram", "values": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 5.0, 6.0, 1.0, 2.0, 3.0, 8.0, 9.0, 9.0, 6.0, 8.0, 20.0, 23.0, 13.0, 41.0, 34.0, 59.0, 87.0, 131.0, 191.0, 254.0, 478.0, 838.0, 1543.0, 881.0, 513.0, 294.0, 199.0, 112.0, 94.0, 58.0, 46.0, 38.0, 23.0, 16.0, 21.0, 12.0, 11.0, 12.0, 9.0, 3.0, 3.0, 5.0, 3.0, 3.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0], "bins": [-303.0, -293.96875, -284.9375, -275.90625, -266.875, -257.84375, -248.8125, -239.78125, -230.75, -221.71875, -212.6875, -203.65625, -194.625, -185.59375, -176.5625, -167.53125, -158.5, -149.46875, -140.4375, -131.40625, -122.375, -113.34375, -104.3125, -95.28125, -86.25, -77.21875, -68.1875, -59.15625, -50.125, -41.09375, -32.0625, -23.03125, -14.0, -4.96875, 4.0625, 13.09375, 22.125, 31.15625, 40.1875, 49.21875, 58.25, 67.28125, 76.3125, 85.34375, 94.375, 103.40625, 112.4375, 121.46875, 130.5, 139.53125, 148.5625, 157.59375, 166.625, 175.65625, 184.6875, 193.71875, 202.75, 211.78125, 220.8125, 229.84375, 238.875, 247.90625, 256.9375, 265.96875, 275.0]}, "gradients/roberta.encoder.layer.4.attention.self.query.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 3.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 7.0, 34.0, 286.0, 4933.0, 805.0, 49.0, 13.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0], "bins": [-1501.0, -1450.28125, -1399.5625, -1348.84375, -1298.125, -1247.40625, -1196.6875, -1145.96875, -1095.25, -1044.53125, -993.8125, -943.09375, -892.375, -841.65625, -790.9375, -740.21875, -689.5, -638.78125, -588.0625, -537.34375, -486.625, -435.90625, -385.1875, -334.46875, -283.75, -233.03125, -182.3125, -131.59375, -80.875, -30.15625, 20.5625, 71.28125, 122.0, 172.71875, 223.4375, 274.15625, 324.875, 375.59375, 426.3125, 477.03125, 527.75, 578.46875, 629.1875, 679.90625, 730.625, 781.34375, 832.0625, 882.78125, 933.5, 984.21875, 1034.9375, 1085.65625, 1136.375, 1187.09375, 1237.8125, 1288.53125, 1339.25, 1389.96875, 1440.6875, 1491.40625, 1542.125, 1592.84375, 1643.5625, 1694.28125, 1745.0]}, "gradients/roberta.encoder.layer.3.attention.self.value.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 5.0, 2.0, 7.0, 11.0, 20.0, 19.0, 35.0, 44.0, 79.0, 104.0, 140.0, 272.0, 482.0, 739.0, 1013.0, 1036.0, 776.0, 546.0, 289.0, 181.0, 112.0, 75.0, 50.0, 32.0, 21.0, 15.0, 8.0, 5.0, 6.0, 3.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-458.5, -441.5859375, -424.671875, -407.7578125, -390.84375, -373.9296875, -357.015625, -340.1015625, -323.1875, -306.2734375, -289.359375, -272.4453125, -255.53125, -238.6171875, -221.703125, -204.7890625, -187.875, -170.9609375, -154.046875, -137.1328125, -120.21875, -103.3046875, -86.390625, -69.4765625, -52.5625, -35.6484375, -18.734375, -1.8203125, 15.09375, 32.0078125, 48.921875, 65.8359375, 82.75, 99.6640625, 116.578125, 133.4921875, 150.40625, 167.3203125, 184.234375, 201.1484375, 218.0625, 234.9765625, 251.890625, 268.8046875, 285.71875, 302.6328125, 319.546875, 336.4609375, 353.375, 370.2890625, 387.203125, 404.1171875, 421.03125, 437.9453125, 454.859375, 471.7734375, 488.6875, 505.6015625, 522.515625, 539.4296875, 556.34375, 573.2578125, 590.171875, 607.0859375, 624.0]}, "gradients/roberta.encoder.layer.3.attention.self.value.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 13.0, 6103.0, 13.0, 2.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-3692.0, -3582.03125, -3472.0625, -3362.09375, -3252.125, -3142.15625, -3032.1875, -2922.21875, -2812.25, -2702.28125, -2592.3125, -2482.34375, -2372.375, -2262.40625, -2152.4375, -2042.46875, -1932.5, -1822.53125, -1712.5625, -1602.59375, -1492.625, -1382.65625, -1272.6875, -1162.71875, -1052.75, -942.78125, -832.8125, -722.84375, -612.875, -502.90625, -392.9375, -282.96875, -173.0, -63.03125, 46.9375, 156.90625, 266.875, 376.84375, 486.8125, 596.78125, 706.75, 816.71875, 926.6875, 1036.65625, 1146.625, 1256.59375, 1366.5625, 1476.53125, 1586.5, 1696.46875, 1806.4375, 1916.40625, 2026.375, 2136.34375, 2246.3125, 2356.28125, 2466.25, 2576.21875, 2686.1875, 2796.15625, 2906.125, 3016.09375, 3126.0625, 3236.03125, 3346.0]}, "gradients/roberta.encoder.layer.3.attention.self.query.lora_B": {"_type": "histogram", "values": [2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 3.0, 2.0, 7.0, 9.0, 6.0, 18.0, 15.0, 31.0, 58.0, 125.0, 243.0, 630.0, 2199.0, 1787.0, 548.0, 204.0, 95.0, 55.0, 20.0, 26.0, 20.0, 9.0, 5.0, 4.0, 4.0, 0.0, 3.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-334.0, -322.03515625, -310.0703125, -298.10546875, -286.140625, -274.17578125, -262.2109375, -250.24609375, -238.28125, -226.31640625, -214.3515625, -202.38671875, -190.421875, -178.45703125, -166.4921875, -154.52734375, -142.5625, -130.59765625, -118.6328125, -106.66796875, -94.703125, -82.73828125, -70.7734375, -58.80859375, -46.84375, -34.87890625, -22.9140625, -10.94921875, 1.015625, 12.98046875, 24.9453125, 36.91015625, 48.875, 60.83984375, 72.8046875, 84.76953125, 96.734375, 108.69921875, 120.6640625, 132.62890625, 144.59375, 156.55859375, 168.5234375, 180.48828125, 192.453125, 204.41796875, 216.3828125, 228.34765625, 240.3125, 252.27734375, 264.2421875, 276.20703125, 288.171875, 300.13671875, 312.1015625, 324.06640625, 336.03125, 347.99609375, 359.9609375, 371.92578125, 383.890625, 395.85546875, 407.8203125, 419.78515625, 431.75]}, "gradients/roberta.encoder.layer.3.attention.self.query.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 2.0, 22.0, 291.0, 4647.0, 1111.0, 52.0, 4.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-1229.0, -1187.609375, -1146.21875, -1104.828125, -1063.4375, -1022.046875, -980.65625, -939.265625, -897.875, -856.484375, -815.09375, -773.703125, -732.3125, -690.921875, -649.53125, -608.140625, -566.75, -525.359375, -483.96875, -442.578125, -401.1875, -359.796875, -318.40625, -277.015625, -235.625, -194.234375, -152.84375, -111.453125, -70.0625, -28.671875, 12.71875, 54.109375, 95.5, 136.890625, 178.28125, 219.671875, 261.0625, 302.453125, 343.84375, 385.234375, 426.625, 468.015625, 509.40625, 550.796875, 592.1875, 633.578125, 674.96875, 716.359375, 757.75, 799.140625, 840.53125, 881.921875, 923.3125, 964.703125, 1006.09375, 1047.484375, 1088.875, 1130.265625, 1171.65625, 1213.046875, 1254.4375, 1295.828125, 1337.21875, 1378.609375, 1420.0]}, "gradients/roberta.encoder.layer.2.attention.self.value.lora_B": {"_type": "histogram", "values": [2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 2.0, 4.0, 4.0, 4.0, 3.0, 5.0, 9.0, 13.0, 22.0, 23.0, 29.0, 55.0, 72.0, 103.0, 155.0, 265.0, 370.0, 512.0, 616.0, 757.0, 749.0, 662.0, 475.0, 381.0, 254.0, 212.0, 114.0, 71.0, 49.0, 38.0, 30.0, 19.0, 11.0, 19.0, 11.0, 6.0, 2.0, 3.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-551.0, -530.6328125, -510.265625, -489.8984375, -469.53125, -449.1640625, -428.796875, -408.4296875, -388.0625, -367.6953125, -347.328125, -326.9609375, -306.59375, -286.2265625, -265.859375, -245.4921875, -225.125, -204.7578125, -184.390625, -164.0234375, -143.65625, -123.2890625, -102.921875, -82.5546875, -62.1875, -41.8203125, -21.453125, -1.0859375, 19.28125, 39.6484375, 60.015625, 80.3828125, 100.75, 121.1171875, 141.484375, 161.8515625, 182.21875, 202.5859375, 222.953125, 243.3203125, 263.6875, 284.0546875, 304.421875, 324.7890625, 345.15625, 365.5234375, 385.890625, 406.2578125, 426.625, 446.9921875, 467.359375, 487.7265625, 508.09375, 528.4609375, 548.828125, 569.1953125, 589.5625, 609.9296875, 630.296875, 650.6640625, 671.03125, 691.3984375, 711.765625, 732.1328125, 752.5]}, "gradients/roberta.encoder.layer.2.attention.self.value.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 5.0, 23.0, 6062.0, 40.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-3232.0, -3150.375, -3068.75, -2987.125, -2905.5, -2823.875, -2742.25, -2660.625, -2579.0, -2497.375, -2415.75, -2334.125, -2252.5, -2170.875, -2089.25, -2007.625, -1926.0, -1844.375, -1762.75, -1681.125, -1599.5, -1517.875, -1436.25, -1354.625, -1273.0, -1191.375, -1109.75, -1028.125, -946.5, -864.875, -783.25, -701.625, -620.0, -538.375, -456.75, -375.125, -293.5, -211.875, -130.25, -48.625, 33.0, 114.625, 196.25, 277.875, 359.5, 441.125, 522.75, 604.375, 686.0, 767.625, 849.25, 930.875, 1012.5, 1094.125, 1175.75, 1257.375, 1339.0, 1420.625, 1502.25, 1583.875, 1665.5, 1747.125, 1828.75, 1910.375, 1992.0]}, "gradients/roberta.encoder.layer.2.attention.self.query.lora_B": {"_type": "histogram", "values": [3.0, 0.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 2.0, 5.0, 4.0, 6.0, 6.0, 13.0, 11.0, 12.0, 23.0, 19.0, 34.0, 43.0, 67.0, 95.0, 168.0, 260.0, 437.0, 808.0, 1230.0, 1084.0, 678.0, 406.0, 239.0, 146.0, 79.0, 72.0, 47.0, 36.0, 18.0, 14.0, 12.0, 13.0, 5.0, 5.0, 4.0, 6.0, 1.0, 4.0, 2.0, 1.0, 2.0, 0.0, 2.0, 4.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0], "bins": [-155.5, -150.46875, -145.4375, -140.40625, -135.375, -130.34375, -125.3125, -120.28125, -115.25, -110.21875, -105.1875, -100.15625, -95.125, -90.09375, -85.0625, -80.03125, -75.0, -69.96875, -64.9375, -59.90625, -54.875, -49.84375, -44.8125, -39.78125, -34.75, -29.71875, -24.6875, -19.65625, -14.625, -9.59375, -4.5625, 0.46875, 5.5, 10.53125, 15.5625, 20.59375, 25.625, 30.65625, 35.6875, 40.71875, 45.75, 50.78125, 55.8125, 60.84375, 65.875, 70.90625, 75.9375, 80.96875, 86.0, 91.03125, 96.0625, 101.09375, 106.125, 111.15625, 116.1875, 121.21875, 126.25, 131.28125, 136.3125, 141.34375, 146.375, 151.40625, 156.4375, 161.46875, 166.5]}, "gradients/roberta.encoder.layer.2.attention.self.query.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 4.0, 10.0, 52.0, 191.0, 973.0, 3017.0, 1493.0, 310.0, 66.0, 12.0, 4.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0], "bins": [-1011.5, -988.328125, -965.15625, -941.984375, -918.8125, -895.640625, -872.46875, -849.296875, -826.125, -802.953125, -779.78125, -756.609375, -733.4375, -710.265625, -687.09375, -663.921875, -640.75, -617.578125, -594.40625, -571.234375, -548.0625, -524.890625, -501.71875, -478.546875, -455.375, -432.203125, -409.03125, -385.859375, -362.6875, -339.515625, -316.34375, -293.171875, -270.0, -246.828125, -223.65625, -200.484375, -177.3125, -154.140625, -130.96875, -107.796875, -84.625, -61.453125, -38.28125, -15.109375, 8.0625, 31.234375, 54.40625, 77.578125, 100.75, 123.921875, 147.09375, 170.265625, 193.4375, 216.609375, 239.78125, 262.953125, 286.125, 309.296875, 332.46875, 355.640625, 378.8125, 401.984375, 425.15625, 448.328125, 471.5]}, "gradients/roberta.encoder.layer.1.attention.self.value.lora_B": {"_type": "histogram", "values": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 5.0, 3.0, 0.0, 3.0, 3.0, 4.0, 13.0, 8.0, 11.0, 16.0, 7.0, 34.0, 42.0, 37.0, 59.0, 72.0, 66.0, 118.0, 137.0, 196.0, 265.0, 400.0, 519.0, 648.0, 745.0, 674.0, 558.0, 388.0, 266.0, 221.0, 148.0, 119.0, 65.0, 64.0, 50.0, 42.0, 28.0, 28.0, 15.0, 20.0, 8.0, 7.0, 7.0, 3.0, 1.0, 3.0, 4.0, 3.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0], "bins": [-480.25, -466.34765625, -452.4453125, -438.54296875, -424.640625, -410.73828125, -396.8359375, -382.93359375, -369.03125, -355.12890625, -341.2265625, -327.32421875, -313.421875, -299.51953125, -285.6171875, -271.71484375, -257.8125, -243.91015625, -230.0078125, -216.10546875, -202.203125, -188.30078125, -174.3984375, -160.49609375, -146.59375, -132.69140625, -118.7890625, -104.88671875, -90.984375, -77.08203125, -63.1796875, -49.27734375, -35.375, -21.47265625, -7.5703125, 6.33203125, 20.234375, 34.13671875, 48.0390625, 61.94140625, 75.84375, 89.74609375, 103.6484375, 117.55078125, 131.453125, 145.35546875, 159.2578125, 173.16015625, 187.0625, 200.96484375, 214.8671875, 228.76953125, 242.671875, 256.57421875, 270.4765625, 284.37890625, 298.28125, 312.18359375, 326.0859375, 339.98828125, 353.890625, 367.79296875, 381.6953125, 395.59765625, 409.5]}, "gradients/roberta.encoder.layer.1.attention.self.value.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 4.0, 13.0, 574.0, 5367.0, 167.0, 6.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], "bins": [-1457.0, -1415.828125, -1374.65625, -1333.484375, -1292.3125, -1251.140625, -1209.96875, -1168.796875, -1127.625, -1086.453125, -1045.28125, -1004.109375, -962.9375, -921.765625, -880.59375, -839.421875, -798.25, -757.078125, -715.90625, -674.734375, -633.5625, -592.390625, -551.21875, -510.046875, -468.875, -427.703125, -386.53125, -345.359375, -304.1875, -263.015625, -221.84375, -180.671875, -139.5, -98.328125, -57.15625, -15.984375, 25.1875, 66.359375, 107.53125, 148.703125, 189.875, 231.046875, 272.21875, 313.390625, 354.5625, 395.734375, 436.90625, 478.078125, 519.25, 560.421875, 601.59375, 642.765625, 683.9375, 725.109375, 766.28125, 807.453125, 848.625, 889.796875, 930.96875, 972.140625, 1013.3125, 1054.484375, 1095.65625, 1136.828125, 1178.0]}, "gradients/roberta.encoder.layer.1.attention.self.query.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 3.0, 1.0, 1.0, 3.0, 4.0, 3.0, 6.0, 12.0, 14.0, 16.0, 20.0, 36.0, 64.0, 148.0, 342.0, 1061.0, 2710.0, 1009.0, 338.0, 124.0, 72.0, 28.0, 23.0, 15.0, 14.0, 14.0, 12.0, 6.0, 5.0, 2.0, 7.0, 2.0, 4.0, 4.0, 5.0, 3.0, 1.0, 3.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-375.5, -360.15625, -344.8125, -329.46875, -314.125, -298.78125, -283.4375, -268.09375, -252.75, -237.40625, -222.0625, -206.71875, -191.375, -176.03125, -160.6875, -145.34375, -130.0, -114.65625, -99.3125, -83.96875, -68.625, -53.28125, -37.9375, -22.59375, -7.25, 8.09375, 23.4375, 38.78125, 54.125, 69.46875, 84.8125, 100.15625, 115.5, 130.84375, 146.1875, 161.53125, 176.875, 192.21875, 207.5625, 222.90625, 238.25, 253.59375, 268.9375, 284.28125, 299.625, 314.96875, 330.3125, 345.65625, 361.0, 376.34375, 391.6875, 407.03125, 422.375, 437.71875, 453.0625, 468.40625, 483.75, 499.09375, 514.4375, 529.78125, 545.125, 560.46875, 575.8125, 591.15625, 606.5]}, "gradients/roberta.encoder.layer.1.attention.self.query.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 9.0, 9.0, 30.0, 80.0, 269.0, 1004.0, 2398.0, 1669.0, 434.0, 141.0, 57.0, 17.0, 9.0, 7.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-321.75, -301.26171875, -280.7734375, -260.28515625, -239.796875, -219.30859375, -198.8203125, -178.33203125, -157.84375, -137.35546875, -116.8671875, -96.37890625, -75.890625, -55.40234375, -34.9140625, -14.42578125, 6.0625, 26.55078125, 47.0390625, 67.52734375, 88.015625, 108.50390625, 128.9921875, 149.48046875, 169.96875, 190.45703125, 210.9453125, 231.43359375, 251.921875, 272.41015625, 292.8984375, 313.38671875, 333.875, 354.36328125, 374.8515625, 395.33984375, 415.828125, 436.31640625, 456.8046875, 477.29296875, 497.78125, 518.26953125, 538.7578125, 559.24609375, 579.734375, 600.22265625, 620.7109375, 641.19921875, 661.6875, 682.17578125, 702.6640625, 723.15234375, 743.640625, 764.12890625, 784.6171875, 805.10546875, 825.59375, 846.08203125, 866.5703125, 887.05859375, 907.546875, 928.03515625, 948.5234375, 969.01171875, 989.5]}, "gradients/roberta.encoder.layer.0.attention.self.value.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 3.0, 1.0, 4.0, 2.0, 1.0, 3.0, 7.0, 5.0, 9.0, 15.0, 7.0, 24.0, 34.0, 35.0, 55.0, 75.0, 98.0, 172.0, 219.0, 364.0, 464.0, 702.0, 721.0, 790.0, 681.0, 498.0, 369.0, 243.0, 173.0, 108.0, 75.0, 51.0, 32.0, 24.0, 15.0, 9.0, 15.0, 10.0, 6.0, 5.0, 2.0, 3.0, 0.0, 3.0, 3.0, 1.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-440.25, -426.08984375, -411.9296875, -397.76953125, -383.609375, -369.44921875, -355.2890625, -341.12890625, -326.96875, -312.80859375, -298.6484375, -284.48828125, -270.328125, -256.16796875, -242.0078125, -227.84765625, -213.6875, -199.52734375, -185.3671875, -171.20703125, -157.046875, -142.88671875, -128.7265625, -114.56640625, -100.40625, -86.24609375, -72.0859375, -57.92578125, -43.765625, -29.60546875, -15.4453125, -1.28515625, 12.875, 27.03515625, 41.1953125, 55.35546875, 69.515625, 83.67578125, 97.8359375, 111.99609375, 126.15625, 140.31640625, 154.4765625, 168.63671875, 182.796875, 196.95703125, 211.1171875, 225.27734375, 239.4375, 253.59765625, 267.7578125, 281.91796875, 296.078125, 310.23828125, 324.3984375, 338.55859375, 352.71875, 366.87890625, 381.0390625, 395.19921875, 409.359375, 423.51953125, 437.6796875, 451.83984375, 466.0]}, "gradients/roberta.encoder.layer.0.attention.self.value.lora_A": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 6.0, 12.0, 48.0, 329.0, 1587.0, 2781.0, 1154.0, 180.0, 25.0, 10.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0], "bins": [-1630.0, -1587.515625, -1545.03125, -1502.546875, -1460.0625, -1417.578125, -1375.09375, -1332.609375, -1290.125, -1247.640625, -1205.15625, -1162.671875, -1120.1875, -1077.703125, -1035.21875, -992.734375, -950.25, -907.765625, -865.28125, -822.796875, -780.3125, -737.828125, -695.34375, -652.859375, -610.375, -567.890625, -525.40625, -482.921875, -440.4375, -397.953125, -355.46875, -312.984375, -270.5, -228.015625, -185.53125, -143.046875, -100.5625, -58.078125, -15.59375, 26.890625, 69.375, 111.859375, 154.34375, 196.828125, 239.3125, 281.796875, 324.28125, 366.765625, 409.25, 451.734375, 494.21875, 536.703125, 579.1875, 621.671875, 664.15625, 706.640625, 749.125, 791.609375, 834.09375, 876.578125, 919.0625, 961.546875, 1004.03125, 1046.515625, 1089.0]}, "gradients/roberta.encoder.layer.0.attention.self.query.lora_B": {"_type": "histogram", "values": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 3.0, 3.0, 3.0, 7.0, 3.0, 4.0, 8.0, 7.0, 14.0, 25.0, 27.0, 25.0, 54.0, 78.0, 95.0, 184.0, 254.0, 393.0, 656.0, 1459.0, 1201.0, 534.0, 366.0, 258.0, 136.0, 106.0, 53.0, 47.0, 36.0, 21.0, 17.0, 16.0, 11.0, 11.0, 4.0, 7.0, 2.0, 2.0, 1.0, 3.0, 3.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-278.0, -269.0, -260.0, -251.0, -242.0, -233.0, -224.0, -215.0, -206.0, -197.0, -188.0, -179.0, -170.0, -161.0, -152.0, -143.0, -134.0, -125.0, -116.0, -107.0, -98.0, -89.0, -80.0, -71.0, -62.0, -53.0, -44.0, -35.0, -26.0, -17.0, -8.0, 1.0, 10.0, 19.0, 28.0, 37.0, 46.0, 55.0, 64.0, 73.0, 82.0, 91.0, 100.0, 109.0, 118.0, 127.0, 136.0, 145.0, 154.0, 163.0, 172.0, 181.0, 190.0, 199.0, 208.0, 217.0, 226.0, 235.0, 244.0, 253.0, 262.0, 271.0, 280.0, 289.0, 298.0]}, "gradients/roberta.encoder.layer.0.attention.self.query.lora_A": {"_type": "histogram", "values": [1.0, 1.0, 5.0, 0.0, 2.0, 4.0, 8.0, 18.0, 12.0, 23.0, 35.0, 68.0, 104.0, 155.0, 191.0, 322.0, 454.0, 598.0, 701.0, 731.0, 715.0, 591.0, 449.0, 323.0, 233.0, 144.0, 92.0, 60.0, 38.0, 22.0, 17.0, 6.0, 7.0, 3.0, 2.0, 3.0, 2.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], "bins": [-79.8125, -75.7119140625, -71.611328125, -67.5107421875, -63.41015625, -59.3095703125, -55.208984375, -51.1083984375, -47.0078125, -42.9072265625, -38.806640625, -34.7060546875, -30.60546875, -26.5048828125, -22.404296875, -18.3037109375, -14.203125, -10.1025390625, -6.001953125, -1.9013671875, 2.19921875, 6.2998046875, 10.400390625, 14.5009765625, 18.6015625, 22.7021484375, 26.802734375, 30.9033203125, 35.00390625, 39.1044921875, 43.205078125, 47.3056640625, 51.40625, 55.5068359375, 59.607421875, 63.7080078125, 67.80859375, 71.9091796875, 76.009765625, 80.1103515625, 84.2109375, 88.3115234375, 92.412109375, 96.5126953125, 100.61328125, 104.7138671875, 108.814453125, 112.9150390625, 117.015625, 121.1162109375, 125.216796875, 129.3173828125, 133.41796875, 137.5185546875, 141.619140625, 145.7197265625, 149.8203125, 153.9208984375, 158.021484375, 162.1220703125, 166.22265625, 170.3232421875, 174.423828125, 178.5244140625, 182.625]}, "eval_metroc": {"matthews_correlation": 0.5601668097291345}}