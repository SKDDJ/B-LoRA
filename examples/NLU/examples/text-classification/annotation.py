""" Finetuning a ğŸ¤— Transformers model for sequence classification on GLUE."""
# ä½¿ç”¨ğŸ¤— Transformersæ¨¡å‹å¯¹GLUEæ•°æ®é›†è¿›è¡Œåºåˆ—åˆ†ç±»çš„å¾®è°ƒã€‚

import argparse
# å¯¼å…¥argparseåº“ï¼Œç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°ã€‚

import logging
# å¯¼å…¥loggingåº“ï¼Œç”¨äºè®°å½•æ—¥å¿—ã€‚

import math
# å¯¼å…¥mathåº“ï¼Œç”¨äºæ•°å­¦è¿ç®—ã€‚

import os
# å¯¼å…¥osåº“ï¼Œç”¨äºå¤„ç†æ–‡ä»¶å’Œç›®å½•ã€‚

import random
# å¯¼å…¥randomåº“ï¼Œç”¨äºç”Ÿæˆéšæœºæ•°ã€‚

import torch
# å¯¼å…¥torchåº“ï¼ŒPyTorchçš„ä¸»è¦åº“ï¼Œç”¨äºæ·±åº¦å­¦ä¹ ã€‚

import datasets
# å¯¼å…¥datasetsåº“ï¼Œç”¨äºåŠ è½½å’Œå¤„ç†æ•°æ®é›†ã€‚

from datasets import load_dataset, load_metric
# ä»datasetsåº“å¯¼å…¥load_datasetå’Œload_metricå‡½æ•°ï¼Œç”¨äºåŠ è½½æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚

from torch.utils.data.dataloader import DataLoader
# ä»torch.utils.data.dataloaderå¯¼å…¥DataLoaderï¼Œç”¨äºåˆ›å»ºæ•°æ®åŠ è½½å™¨ã€‚

from tqdm.auto import tqdm
# ä»tqdm.autoå¯¼å…¥tqdmï¼Œç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡ã€‚

import transformers
# å¯¼å…¥transformersåº“ï¼Œç”¨äºè®¿é—®é¢„è®­ç»ƒæ¨¡å‹å’Œå·¥å…·ã€‚

from accelerate import Accelerator
# ä»accelerateåº“å¯¼å…¥Acceleratorï¼Œç”¨äºåŠ é€Ÿæ¨¡å‹è®­ç»ƒã€‚

from transformers import (
    AdamW,
    AutoConfig,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    PretrainedConfig,
    SchedulerType,
    default_data_collator,
    get_scheduler,
    set_seed,
)
# ä»transformersåº“å¯¼å…¥å¤šä¸ªå·¥å…·å’Œç±»ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨ã€é…ç½®ã€æ¨¡å‹ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨ã€è°ƒåº¦å™¨ç­‰ã€‚

logger = logging.getLogger(__name__)
# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨ã€‚

task_to_keys = {
    "cola": ("sentence", None),
    "mnli": ("premise", "hypothesis"),
    "mrpc": ("sentence1", "sentence2"),
    "qnli": ("question", "sentence"),
    "qqp": ("question1", "question2"),
    "rte": ("sentence1", "sentence2"),
    "sst2": ("sentence", None),
    "stsb": ("sentence1", "sentence2"),
    "wnli": ("sentence1", "sentence2"),
}
# å®šä¹‰ä¸åŒä»»åŠ¡å¯¹åº”çš„è¾“å…¥é”®å€¼ã€‚

def parse_args():
    # å®šä¹‰ä¸€ä¸ªè§£æå‘½ä»¤è¡Œå‚æ•°çš„å‡½æ•°ã€‚
    parser = argparse.ArgumentParser(description="Finetune a transformers model on a text classification task")
    # åˆ›å»ºä¸€ä¸ªArgumentParserå¯¹è±¡ï¼Œç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•°ã€‚
    parser.add_argument(
        "--task_name",
        type=str,
        default=None,
        help="The name of the glue task to train on.",
        choices=list(task_to_keys.keys()),
    )
    # æ·»åŠ ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°ï¼Œç”¨äºæŒ‡å®šGLUEä»»åŠ¡çš„åç§°ã€‚
    # çœç•¥å‰©ä½™çš„parser.add_argumentä»£ç å—ï¼Œè¿™äº›å—ä¸ºè„šæœ¬æ·»åŠ ä¸åŒçš„å‘½ä»¤è¡Œé€‰é¡¹ã€‚
    # ...
    
    args = parser.parse_args()
    # è§£æå‘½ä»¤è¡Œè¾“å…¥çš„å‚æ•°ã€‚

    # Sanity checks
    # è¿›è¡Œä¸€äº›åŸºæœ¬çš„æ£€æŸ¥ã€‚
    if args.task_name is None and args.train_file is None and args.validation_file is None:
        raise ValueError("Need either a task name or a training/validation file.")
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»åŠ¡åç§°æˆ–è®­ç»ƒ/éªŒè¯æ–‡ä»¶ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸ã€‚
    else:
        # æ£€æŸ¥è®­ç»ƒå’ŒéªŒè¯æ–‡ä»¶çš„æ‰©å±•åæ˜¯å¦æ­£ç¡®ã€‚
        if args.train_file is not None:
            extension = args.train_file.split(".")[-1]
            assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
        if args.validation_file is not None:
            extension = args.validation_file.split(".")[-1]
            assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."

    if args.output_dir is not None:
        os.makedirs(args.output_dir, exist_ok=True)
    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼Œåˆ™åˆ›å»ºè¯¥ç›®å½•ã€‚

    return args
    # è¿”å›è§£æå¾—åˆ°çš„å‚æ•°ã€‚
    
    
    
    
def main():
    # å®šä¹‰ä¸»å‡½æ•°ã€‚
    args = parse_args()
    # è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚

    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.
    # åˆå§‹åŒ–åŠ é€Ÿå™¨ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è®©åŠ é€Ÿå™¨å¤„ç†è®¾å¤‡æ”¾ç½®ã€‚
    accelerator = Accelerator()

    # Make one log on every process with the configuration for debugging.
    # ä¸ºè°ƒè¯•é…ç½®åœ¨æ¯ä¸ªè¿›ç¨‹ä¸Šè®°å½•ä¸€æ¬¡æ—¥å¿—ã€‚
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    logger.info(accelerator.state)

    # Setup logging, we only want one process per machine to log things on the screen.
    # è®¾ç½®æ—¥å¿—ï¼Œæˆ‘ä»¬åªå¸Œæœ›æ¯å°æœºå™¨ä¸Šæœ‰ä¸€ä¸ªè¿›ç¨‹åœ¨å±å¹•ä¸Šè®°å½•æ—¥å¿—ã€‚
    # accelerator.is_local_main_process is only True for one process per machine.
    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)
    if accelerator.is_local_main_process:
        datasets.utils.logging.set_verbosity_warning()
        transformers.utils.logging.set_verbosity_info()
    else:
        datasets.utils.logging.set_verbosity_error()
        transformers.utils.logging.set_verbosity_error()

    # If passed along, set the training seed now.
    # å¦‚æœæä¾›äº†ï¼Œç°åœ¨è®¾ç½®è®­ç»ƒç§å­ã€‚
    if args.seed is not None:
        set_seed(args.seed)

    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)
    # or specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).
    # è·å–æ•°æ®é›†ï¼šä½ å¯ä»¥æä¾›è‡ªå·±çš„CSV/JSONè®­ç»ƒå’Œè¯„ä¼°æ–‡ä»¶ï¼ˆè§ä¸‹æ–‡ï¼‰ï¼Œæˆ–è€…æŒ‡å®šä¸€ä¸ªGLUEåŸºå‡†ä»»åŠ¡ï¼ˆæ•°æ®é›†å°†è‡ªåŠ¨ä»datasets Hubä¸‹è½½ï¼‰ã€‚

    # For CSV/JSON files, this script will use as labels the column called 'label' and as pair of sentences the
    # sentences in columns called 'sentence1' and 'sentence2' if such column exists or the first two columns not named
    # label if at least two columns are provided.
    # å¯¹äºCSV/JSONæ–‡ä»¶ï¼Œæ­¤è„šæœ¬å°†ä½¿ç”¨åä¸º'label'çš„åˆ—ä½œä¸ºæ ‡ç­¾ï¼Œå¦‚æœå­˜åœ¨è¿™æ ·çš„åˆ—ï¼Œåˆ™ä½¿ç”¨åä¸º'sentence1'å’Œ'sentence2'çš„åˆ—ä½œä¸ºå¥å­å¯¹ï¼Œ
    # æˆ–è€…å¦‚æœæä¾›äº†è‡³å°‘ä¸¤åˆ—ï¼Œåˆ™ä½¿ç”¨å‰ä¸¤åˆ—ï¼ˆä¸å‘½åä¸º'label'çš„åˆ—ï¼‰ã€‚

    # If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this
    # single column. You can easily tweak this behavior (see below)
    # å¦‚æœCSV/JSONæ–‡ä»¶åªåŒ…å«ä¸€ä¸ªéæ ‡ç­¾åˆ—ï¼Œåˆ™è„šæœ¬å°†å¯¹è¿™ä¸ªå•ç‹¬çš„åˆ—è¿›è¡Œå•å¥åˆ†ç±»ã€‚ä½ å¯ä»¥å¾ˆå®¹æ˜“åœ°è°ƒæ•´è¿™ä¸ªè¡Œä¸ºï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚

    # In distributed training, the load_dataset function guarantee that only one local process can concurrently
    # download the dataset.
    # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œload_datasetå‡½æ•°ä¿è¯åªæœ‰ä¸€ä¸ªæœ¬åœ°è¿›ç¨‹å¯ä»¥åŒæ—¶ä¸‹è½½æ•°æ®é›†ã€‚
    if args.task_name is not None:
        # Downloading and loading a dataset from the hub.
        # ä»Hubä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†ã€‚
        raw_datasets = load_dataset("glue", args.task_name)
    else:
        # Loading the dataset from local csv or json file.
        # ä»æœ¬åœ°csvæˆ–jsonæ–‡ä»¶åŠ è½½æ•°æ®é›†ã€‚
        data_files = {}
        if args.train_file is not None:
            data_files["train"] = args.train_file
        if args.validation_file is not None:
            data_files["validation"] = args.validation_file
        extension = (args.train_file if args.train_file is not None else args.valid_file).split(".")[-1]
        raw_datasets = load_dataset(extension, data_files=data_files)

    # See more about loading any type of standard or custom dataset at
    # https://huggingface.co/docs/datasets/loading_datasets.html.
    # æœ‰å…³åŠ è½½ä»»ä½•ç±»å‹çš„æ ‡å‡†æˆ–è‡ªå®šä¹‰æ•°æ®é›†çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®https://huggingface.co/docs/datasets/loading_datasets.htmlã€‚

    # Labels
    # æ ‡ç­¾å¤„ç†éƒ¨åˆ†ã€‚
    if args.task_name is not None:
        is_regression = args.task_name == "stsb"
        if not is_regression:
            label_list = raw_datasets["train"].features["label"].names
            num_labels = len(label_list)
        else:
            num_labels = 1
    else:
        # Trying to have good defaults here, don't hesitate to tweak to your needs.
        # å°è¯•åœ¨è¿™é‡Œæœ‰å¥½çš„é»˜è®¤è®¾ç½®ï¼Œæ ¹æ®éœ€è¦éšæ—¶è°ƒæ•´ã€‚
        is_regression = datasets["train"].features["label"].dtype in ["float32", "float64"]
        if is_regression:
            num_labels = 1
        else:
            # A useful fast method:
            # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique
            label_list = datasets["train"].unique("label")
            label_list.sort()  # Let's sort it for determinism
            num_labels = len(label_list)

    # Load pretrained model and tokenizer
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œ.from_pretrainedæ–¹æ³•ä¿è¯åªæœ‰ä¸€ä¸ªæœ¬åœ°è¿›ç¨‹å¯ä»¥åŒæ—¶ä¸‹è½½æ¨¡å‹å’Œè¯æ±‡è¡¨ã€‚
    config = AutoConfig.from_pretrained(args.model_name_or_path, 
                                        num_labels=num_labels, 
                                        finetuning_task=args.task_name,
                                        apply_lora=args.apply_lora,
                                        lora_alpha=args.lora_alpha,
                                        lora_r=args.lora_r,
                                       )
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)
    model = AutoModelForSequenceClassification.from_pretrained(
        args.model_name_or_path,
        from_tf=bool(".ckpt" in args.model_name_or_path),
        config=config,
    )


    # Preprocessing the datasets
    # é¢„å¤„ç†æ•°æ®é›†
    if args.task_name is not None:
        sentence1_key, sentence2_key = task_to_keys[args.task_name]
    else:
        # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.
        # æˆ‘ä»¬å°è¯•æœ‰ä¸€äº›å¥½çš„é»˜è®¤è®¾ç½®ï¼Œä½†ä¸è¦çŠ¹è±«æ ¹æ®æ‚¨çš„ç”¨ä¾‹è¿›è¡Œè°ƒæ•´ã€‚
        non_label_column_names = [name for name in datasets["train"].column_names if name != "label"]
        if "sentence1" in non_label_column_names and "sentence2" in non_label_column_names:
            sentence1_key, sentence2_key = "sentence1", "sentence2"
        else:
            if len(non_label_column_names) >= 2:
                sentence1_key, sentence2_key = non_label_column_names[:2]
            else:
                sentence1_key, sentence2_key = non_label_column_names[0], None

    # Some models have set the order of the labels to use, so let's make sure we do use it.
    # ä¸€äº›æ¨¡å‹å·²ç»è®¾ç½®äº†æ ‡ç­¾çš„ä½¿ç”¨é¡ºåºï¼Œè®©æˆ‘ä»¬ç¡®ä¿æˆ‘ä»¬ä½¿ç”¨å®ƒã€‚
    label_to_id = None
    if (
        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id
        and args.task_name is not None
        and not is_regression
    ):
        # Some have all caps in their config, some don't.
        # ä¸€äº›é…ç½®ä¸­çš„æ ‡ç­¾æ˜¯å¤§å†™ï¼Œæœ‰äº›ä¸æ˜¯ã€‚
        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}
        if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):
            logger.info(
                f"The configuration of the model provided the following label correspondence: {label_name_to_id}. "
                "Using it!"
            )
            label_to_id = {i: label_name_to_id[label_list[i]] for i in range(num_labels)}
        else:
            logger.warn(
                "Your model seems to have been trained with labels, but they don't match the dataset: ",
                f"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}."
                "\nIgnoring the model labels as a result.",
            )
    elif args.task_name is None:
        label_to_id = {v: i for i, v in enumerate(label_list)}

    padding = "max_length" if args.pad_to_max_length else False

    def preprocess_function(examples):
        # Tokenize the texts
        # åˆ†è¯å¤„ç†æ–‡æœ¬
        texts = (
            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])
        )
        result = tokenizer(*texts, padding=padding, max_length=args.max_length, truncation=True)

        if "label" in examples:
            if label_to_id is not None:
                # Map labels to IDs (not necessary for GLUE tasks)
                # å°†æ ‡ç­¾æ˜ å°„åˆ°IDï¼ˆGLUEä»»åŠ¡ä¸éœ€è¦ï¼‰
                result["labels"] = [label_to_id[l] for l in examples["label"]]
            else:
                # In all cases, rename the column to labels because the model will expect that.
                # åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œå°†åˆ—é‡å‘½åä¸ºlabelsï¼Œå› ä¸ºæ¨¡å‹ä¼šæœŸæœ›è¿™æ ·ã€‚
                result["labels"] = examples["label"]
        return result

    processed_datasets = raw_datasets.map(
        preprocess_function, batched=True, remove_columns=raw_datasets["train"].column_names
    )

    train_dataset = processed_datasets["train"]
    eval_dataset = processed_datasets["validation_matched" if args.task_name == "mnli" else "validation"]

    # Log a few random samples from the training set:
    # è®°å½•è®­ç»ƒé›†ä¸­çš„ä¸€äº›éšæœºæ ·æœ¬ï¼š
    for index in random.sample(range(len(train_dataset)), 3):
        logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")

    # DataLoaders creation:
    # åˆ›å»ºDataLoadersï¼š
    if args.pad_to_max_length:
        # If padding was already done ot max length, we use the default data collator that will just convert everything
        # to tensors.
        # å¦‚æœå·²ç»å¯¹æœ€å¤§é•¿åº¦è¿›è¡Œäº†å¡«å……ï¼Œæˆ‘ä»¬ä½¿ç”¨é»˜è®¤çš„data collatorï¼Œå®ƒä¼šå°†æ‰€æœ‰å†…å®¹è½¬æ¢ä¸ºå¼ é‡ã€‚
        data_collator = default_data_collator
    else:
        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of
        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple
        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).
        # å¦åˆ™ï¼Œ`DataCollatorWithPadding`å°†ä¸ºæˆ‘ä»¬åº”ç”¨åŠ¨æ€å¡«å……ï¼ˆé€šè¿‡å¡«å……åˆ°ä¼ é€’çš„æ ·æœ¬çš„æœ€å¤§é•¿åº¦ï¼‰ã€‚å½“ä½¿ç”¨æ··åˆç²¾åº¦æ—¶ï¼Œæˆ‘ä»¬æ·»åŠ `pad_to_multiple_of=8`
        # æ¥å°†æ‰€æœ‰å¼ é‡å¡«å……ä¸º8çš„å€æ•°ï¼Œè¿™å°†å¯ç”¨åœ¨å…·æœ‰è®¡ç®—èƒ½åŠ›>=7.5ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šä½¿ç”¨Tensor Coresã€‚
        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))

    train_dataloader = DataLoader(
        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size
    )
    # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œå‚æ•°åŒ…æ‹¬æ•°æ®é›†ã€æ˜¯å¦éšæœºæ‰“ä¹±ã€æ•°æ®æ•´ç†å‡½æ•°å’Œæ¯è®¾å¤‡æ‰¹å¤„ç†å¤§å°ã€‚

    eval_dataloader = DataLoader(
        eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size
    )
    # åˆ›å»ºè¯„ä¼°æ•°æ®åŠ è½½å™¨ï¼Œå‚æ•°åŒ…æ‹¬æ•°æ®é›†ã€æ•°æ®æ•´ç†å‡½æ•°å’Œæ¯è®¾å¤‡æ‰¹å¤„ç†å¤§å°ã€‚

    for name, params in model.named_parameters():
        if 'lora_A' in name or 'lora_B' in name:
            params.requires_grad = True
        else:
            params.requires_grad = False
    # éå†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œå¯¹äºåŒ…å«'lora_A'æˆ–'lora_B'çš„å‚æ•°å¯ç”¨æ¢¯åº¦ï¼Œå…¶ä½™å‚æ•°ç¦ç”¨æ¢¯åº¦ã€‚

    # Optimizer
    # é…ç½®ä¼˜åŒ–å™¨
    # Split weights in two groups, one with weight decay and the other not.
    # å°†æƒé‡åˆ†ä¸ºä¸¤ç»„ï¼Œä¸€ç»„ä½¿ç”¨æƒé‡è¡°å‡ï¼Œå¦ä¸€ç»„ä¸ä½¿ç”¨ã€‚
    no_decay = ["bias", "LayerNorm.weight"]
    lora_param_names = [n for n, p in model.named_parameters() if 'lora_A' in n or 'lora_B' in n]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if n in lora_param_names],
            "weight_decay": args.weight_decay,  # æˆ–è€…ä½ å¸Œæœ›ä¸ºLoRAå‚æ•°è®¾å®šçš„å…¶ä»–å€¼
        },
        {
            "params": [p for n, p in model.named_parameters() if n not in lora_param_names and n not in no_decay],
            "weight_decay": 0.0,  # éLoRAå‚æ•°çš„æƒé‡è¡°å‡
        },
    ]
    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)
    # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œé…ç½®ä¸åŒçš„å‚æ•°ç»„ã€‚

    # Prepare everything with our `accelerator`.
    # ä½¿ç”¨`accelerator`å‡†å¤‡æ‰€æœ‰å†…å®¹ã€‚
    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader
    )

    # Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be
    # shorter in multiprocess)
    # æ³¨æ„ -> åœ¨ä¸‹é¢è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨çš„é•¿åº¦ä¹‹å‰éœ€è¦å‡†å¤‡å¥½å®ƒï¼ˆå› ä¸ºåœ¨å¤šè¿›ç¨‹ä¸­å®ƒçš„é•¿åº¦ä¼šæ›´çŸ­ï¼‰ã€‚

    # Scheduler and math around the number of training steps.
    # è®¡åˆ’å™¨å’Œå›´ç»•è®­ç»ƒæ­¥éª¤æ•°é‡çš„æ•°å­¦è®¡ç®—ã€‚
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if args.max_train_steps is None:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    else:
        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
    # æ ¹æ®æ¯ä¸ªepochçš„æ›´æ–°æ­¥éª¤æ•°å’Œæ€»è®­ç»ƒæ­¥éª¤è®¡ç®—æ€»è®­ç»ƒepochæ•°ã€‚

    warmup_ratio = 0.06
    n_steps = len(train_dataloader) * args.num_train_epochs
    warmup_steps = warmup_ratio * n_steps
    # è®¡ç®—é¢„çƒ­æ­¥éª¤æ•°ã€‚

    lr_scheduler = get_scheduler(
        name=args.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=args.max_train_steps,
    )
    # é…ç½®å­¦ä¹ ç‡è®¡åˆ’å™¨ã€‚

    # Get the metric function
    # è·å–è¯„ä¼°æŒ‡æ ‡å‡½æ•°
    if args.task_name is not None:
        metric = load_metric("glue", args.task_name)

    # Train!
    # å¼€å§‹è®­ç»ƒï¼
    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
    # è®¡ç®—æ€»æ‰¹å¤„ç†å¤§å°ã€‚

    logger.info("***** Running training *****")
    logger.info(f"  Num examples = {len(train_dataset)}")
    logger.info(f"  Num Epochs = {args.num_train_epochs}")
    logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
    logger.info(f"  Total optimization steps = {args.max_train_steps}")
    # è®°å½•è®­ç»ƒè¿‡ç¨‹çš„ç›¸å…³ä¿¡æ¯ã€‚

    # Only show the progress bar once on each machine.
    # æ¯å°æœºå™¨ä¸Šåªæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æ¡ã€‚
    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
    completed_steps = 0
    for epoch in range(args.num_train_epochs):
        model.train()
        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ã€‚
        for step, batch in enumerate(train_dataloader):
            outputs = model(**batch)
            loss = outputs.loss
            loss = loss / args.gradient_accumulation_steps
            accelerator.backward(loss)
            # è®¡ç®—æŸå¤±å¹¶è¿›è¡Œåå‘ä¼ æ’­ã€‚

            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()
                progress_bar.update(1)
                completed_steps += 1
            # æ›´æ–°ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è®¡åˆ’å™¨ï¼Œå¹¶é‡ç½®æ¢¯åº¦ã€‚

            if completed_steps >= args.max_train_steps:
                break
        # å®Œæˆæ‰€æœ‰è®­ç»ƒæ­¥éª¤åé€€å‡ºè®­ç»ƒå¾ªç¯ã€‚

        model.eval()
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ã€‚
        for step, batch in enumerate(eval_dataloader):
            outputs = model(**batch)
            predictions = outputs.logits.argmax(dim=-1)
            metric.add_batch(
                predictions=accelerator.gather(predictions),
                references=accelerator.gather(batch["labels"]),
            )
        # åœ¨è¯„ä¼°æ•°æ®ä¸Šè¿è¡Œæ¨¡å‹å¹¶æ”¶é›†é¢„æµ‹ç»“æœã€‚

        eval_metric = metric.compute()
        logger.info(f"epoch {epoch}: {eval_metric}")
    # è®¡ç®—å¹¶è®°å½•æ¯ä¸ªepochçš„è¯„ä¼°æŒ‡æ ‡ã€‚

    if args.output_dir is not None:
        accelerator.wait_for_everyone()
        unwrapped_model = accelerator.unwrap_model(model)
        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)
    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼Œä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚

    if args.task_name == "mnli":
        # Final evaluation on mismatched validation set
        # åœ¨ä¸åŒ¹é…çš„éªŒè¯é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
        eval_dataset = processed_datasets["validation_mismatched"]
        eval_dataloader = DataLoader(
            eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size
        )
        eval_dataloader = accelerator.prepare(eval_dataloader)

        model.eval()
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ã€‚
        for step, batch in enumerate(eval_dataloader):
            outputs = model(**batch)
            predictions = outputs.logits.argmax(dim=-1)
            metric.add_batch(
                predictions=accelerator.gather(predictions),
                references=accelerator.gather(batch["labels"]),
            )
        # åœ¨ä¸åŒ¹é…çš„éªŒè¯æ•°æ®ä¸Šè¿è¡Œæ¨¡å‹å¹¶æ”¶é›†é¢„æµ‹ç»“æœã€‚

        eval_metric = metric.compute()
        logger.info(f"mnli-mm: {eval_metric}")
    # è®¡ç®—å¹¶è®°å½•mnliä»»åŠ¡çš„ä¸åŒ¹é…éªŒè¯é›†çš„è¯„ä¼°æŒ‡æ ‡ã€‚

if __name__ == "__main__":
    main()
    # å¦‚æœè„šæœ¬æ˜¯ä½œä¸ºä¸»ç¨‹åºè¿è¡Œï¼Œåˆ™æ‰§è¡Œmainå‡½æ•°ã€‚
